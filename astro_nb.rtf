{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 Menlo-Regular;
\f3\fnil\fcharset0 HelveticaNeue;\f4\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red0\green0\blue233;
\red242\green242\blue242;\red251\green2\blue7;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c0\c0;\cssrgb\c0\c0\c93333;
\csgray\c95825;\cssrgb\c100000\c14913\c0;\cssrgb\c100000\c100000\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww14860\viewh13060\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Astro Notebook\
\
size: 91847 x 4000\
\
2/27/19\
\
updating reproduce_figs; now x is no longer a flat array\
\
3/3/19\
\
new code was running too slow\
tried doing plate averages using boundary indices; that sped up original code by maybe factor of 2\
new code still taking long time\
the boundary part runs in fraction of a second on original code\
\
3/7/19\
\
check timing of averaging\
1d: 0.02 s new vs. 0.44 s old\
2d: 6.7 s new vs. too long\
timing ivar conversion:\
	1.3s new vs. 9.4s old\
timing elementwise multiplications:\
new 1d: 3 min 26 s\
old 1d (for loop): out of disk space\
new 2d: 3 min 45 s\
time entire 2d code, after updating the 3 things above:\
	7 min 53 s\
	ran again: 3 min 58 s\
time entire 1d code, after updates:\
	2 min 10 s\
result: plot is almost linear, increasing from left to right. Does not seem right.\
check: use same code, but tile the previous i100\
result: get wrong plot, max around 0.1, flat on right side\
BUT: without masking 1d, get same plot\
conclusion: difference due to masking\
\
3/9/19\
\
the new plots are not looking right\
\
check tao and i100: values should be around .1 for E(B-V), 0.3 for tao, and same for i100\
E(B-V): avg .056, generally between 0.01 and 0.1\
tao: avg 0.16, most under 0.3\
i100:\
norm\cf2 \cb3 al: \CocoaLigature0 [2.56069 3.91738 2.99216 ... 2.46197 2.37942 2.31202]\
high w: [2.46605234 3.68632214 2.85999248 ... 2.39359553 2.31382606 2.24888524]\
low w: [2.25698774 3.20141494 2.57286533 ... 2.23931074 2.16574549 2.10627097]\
high: differ by 0.2 on avg\
low: differ by 0.48 on avg\
all seems fine\
found mistake\
	was using tao instead of i100 in the main code\
fixed it, plot looks good. higher everywhere, a bit more difference on the left.\
\
3/11/19\
\
small-tao limit: looks identical\
FK5 coordinates: they are J2000 equatorial coordinates\
\
3/19/19\
\
compare jiaoyue\'92s code to mine, see where difference is\
convert mine to scatterplot\
plan: check intermediate steps for discrepancies, esp. check alphas and x and y before binning\
plan: change her code so values should be exactly same\
\
3/25/19\
\
check her sigma values (before binning)\
result: on the order of 10^-15 for last couple\
fixed units, changed conversions to be same as mine. now std are around 0.1\
y looks same, x starts same, ends different\
fixed boundary error in my code, now x looks same\
verified both codes still gives same plots\
alphas before binning are different\
checked ivar, looks same\
check code to calculate alpha: same results when both in same code\
\
4/1/19\
\
removing line ivar *= (ivar>0) did nothing (jiaoyue does not have this line)\
changing to her masking method made big difference (fig: blake_alt_mask)\
alphas from my code with alt mask:\
426, 469, 530, 615\'85\
alphas from my code with my mask:\
652, 751, 911, 1211\'85\
alphas from jiaoyue code:\
425, 468, 529, 614\'85\
(result: main difference is due to mask)\
alphas from jiaoyue code, using my mask:\
651, 749, 910, 1208\
\
4/4/19\
\
saved and checked alphas. max fractional difference is 0.7 percent. maybe because of different correction factor for units.\
result: the main discrepancy was from the masking\
(I was neglecting masked values when I averaged over the plate)\
I got E(B-V) for boss\
\
4/5/19\
\
record how much time some sections take:\
15s: loading data\
26s: construct y from flambda\
10s: unit conversions (x, ivar)\
225s: computes alphas from x, y, ivar\
cleaned up code a bit\
\
4/29/19\
\
transferred clean-up edits to 2d code\
converted 2d code so everything is 32-bit\
now 2d runs in 195 s (3 min 15s)\
changed everything to scatterplots. still need to test.\
added back line: ivar *= (ivar>0) to 1d code	\
\
4/30/19\
\
tried binning with normal averages, still see peaks. probably because of masking.\
changed 1d code to float32\
scatterplots all seem to be working\
I don\'92t think peaks in middle differ significantly\
jiaoyue\'92s plot has a dip in the right side of the plot, probably mine cuts off before that\
\
5/1/19\
\
issues with 2d plot were because I forgot the 2d plot should be different from the 1d\
\
5/2/19\
\
Tried to get the IDL code working\
started translating to python\
some functions seem to not exist, and I don\'92t know which data files to use.\
(looked at IRIS website, not sure what to download, or what is irispro)\
\
5/5/19\
\
Try to get values from SFD maps\
found fits files on Harvard Dataverse (
\f1 \cf4 \cb1 \expnd0\expndtw0\kerning0
\ul \ulc4 \CocoaLigature1 https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EWCNL5
\f0 \cf2 \cb3 \kerning1\expnd0\expndtw0 \ulnone \CocoaLigature0 \
along with C code, but the C code is not working\
changed name of getline() function so compiler wouldn\'92t complain\
changed void main -> int main, but those were just warnings.\
to compile:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 	gcc dust_getval.c subs_asciifile.c subs_fits.c subs_inoutput.c subs_lambert.c subs_memory.c
\f0\fs24 \cf2 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 to run:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 	./a.out map=I100 infile='/Users/blakechellew/Documents/DustProject/SFD_Maps/infile.txt'  ipath='/Users/blakechellew/Documents/DustProject/SFD_Maps' interp=y
\f0\fs24 \cf2 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 there is an error somewhere, causing \'93abort trap: 6\'94. Trying to track down which line.\
removed a problematic line . . . but getting 0 as output now.\
\
5/7/19\
\
yesterday made progress on converting the IDL code\
still a couple functions I don\'92t have, and a couple I need to implement\
now the IDL-translated code runs, but output is wrong. At least order of magnitude is correct.\
\
5/9/19\
\
fixed bug where coordinate transformation was not happening\
now coordinates are transforming correctly\
saved original i100 values as i100_1d.npy\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 LinregressResult(slope=0.32793244899092727, intercept=10.730185218925781, rvalue=0.33364407734512602, pvalue=0.0, stderr=0.0030573581360707697)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 First 50 values were much better (good correlation, not quite 0.5 slope):\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 LinregressResult(slope=0.66590314934272488, intercept=1.7588807181490602, rvalue=0.92621207273714967, pvalue=5.6946262432240716e-22, stderr=0.03912233057121501)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 \
5/12/19\
\
checking to make sure it\'92s getting the right data files.\
looks like it is.\
\
5/14/19\
\
trying a astropy.fits function (WCS) to replace the ad2xy function\
replaced cdelt3 = 0 with cdelt3 = 1, because complained about singular matrix\
still get some good correlation, but some outputs are NaN.\
plotted vs. previous i100, looks similar to other plot.\
output:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 LinregressResult(slope=0.31884597822126776, intercept=8.6983945920347185, rvalue=0.18412825587405438, pvalue=0.0, stderr=0.0056162064408993573)
\f0\fs24 \cf2 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 tried converting l to range -180 to 180, didn\'92t help\
file 198 is the one that outputs nan, I assume it just doesn\'92t have any of the l and b in it.\
points 0 through 120 look like 2 separate groups, both highly correlated, but less when together.\
\
5/15/19:\
\
plot: correlation_v1.png is IRIS vs SFD i100, using the copypasted ad2y\
plot: first_120.png is first 120 of above plot, but using WCS instead\
wrote my own version of ad2xy, using formulas for tangent projection.\
seems to work at least as well as the other ones.\
\
5/19/19:\
\
started to check first set of actual IRIS coordinates from Brandt\
they kind of line up for first few values\
\
5/20/19:\
\
removed file 198, temporarily\
(seems to be returning bad values for a lot of the points)\
plot: first_150_actual (compare first 150 values in test file)\
seems like one point at a time works ok (except for file 198). but when we try to\
search an image where the query coordinates are out of range, it fails.\
fixed issue with interpolation, where it was extrapolating\
changed code that decides which IRIS files to pull; I think it was doing it wrong.\
plot: first_10000_actual (compare first 10000 values in test file)\
	looks good, more spread than I expect though.\
plot: correlation_v3_1 through correlation_v3_4\
	various zoom levels of my calculated IRIS values vs SFD i100 values.\
	generally good but there is a tail going upwards.\
there are clearly two different distributions here. I wonder why.\
\
5/22/19:\
\
fk4 coordinates give clearly better results compared to fk5\
	(for comparing to exact IRIS)\
using original file selection code, get much better linearity, but most points fail\
plot: actual_10000_mine (using my file selection; no bad values)\
plot: actual_10000_orig (using original file selection; 8998 bad values)\
tried 2nd set of coordinates, assuming same i10 values.\
	similar results. (they are FK5, I think)\
\
5/23/19:\
\
verified reason for original code not picking up some files:\
	wide RA range and small DEC range in the data. this is a case not checked by the original code.\
commented out all the files that are 0 to 360;\
	didn\'92t make a difference. still successful on all files.\
\
5/25/19:\
\
replaced my ad2xy function with WCS\
now correlation with actual IRIS values is VERY good.\
plot: actual_10000_52519 (my code vs. actual IRIS, looks very good now)\
comparing to SFD still looks like 2 distributions\
plot: IRISvsSFD_52519 (my IRIS predictions vs SFD)\
checked more ranges of coordinates against actual IRIS\
	100k-110k, 200k-210k, 700k-710k, 900k-end\
checked 2nd actual IRIS data again. Comparable accuracy.\
For both actual datasets, slope is 0.635, probably skewed due to higher values.\
\
5/26/19:\
\
tried rounding x, y to nearest int. did not remove scatter.\
changed WCS origin to 0; that removed most of scatter (r = 0.999)\
changed interp to linear (from cubic) -> r = 0.99999999998\
	so little scatter, can barely see points behind line\
tried rounding to nearest int again, that increased scatter.\
checked a few coordinate ranges against actual IRIS, all look good\
plot: actual_10000_52619\
SFD comparison looks similar still\
plot: sfd_10000_52619\
picked a couple points from each distribution, didn\'92t notice any pattern\
\
6/3/19:\
\
confirmed that 2nd set of coordinates is same, but FK5 (no scatter)\
updated reproduce_figs.py to handle both 1d and 2d modes.\
saved alphas for 1d and 2d\
	alphas_1d.npy, alphas_1d_stds.npy\
	alphas_2d.npy, alphas_2d_stds.npy\
code: scatterplot.py (to plot the alphas)\
	peaks a bit too high.\
got i100 at all SFD locations\
	plots: sfd_all_6319, sfd_all_6319_2\
numpy array: iris_i100_at_sfd.npy\
	iris i100 values at location of SFD\
incorporate tao:\
	i100_iris_tao.npy\
calculate new alphas:\
	alphas_iris.npy, alphas_iris_stds,npy\
	scatterplot looks slightly lower\
\
6/4/19:\
\
started finding equivalent widths of emission lines\
\
6/5/19\
\
calculated correction for 4000 A break\
\
6/6/19:\
\
check wavelength array, histogram of element-to-element differences. not constant.\
started to update method of calculating line widths (no more interpolation,\
	use background specified in paper)\
\
6/7/19:\
\
added error estimation\
\
6/12/19:\
\
got alphas for iris WITHOUT tao adjustment\
alphas_iris_1d.npy\
\
6/13/19:\
\
made some nice plots for the 134 paper\
\
6/15/19:\
\
spent a lot of time (12 hr) working on the 134 paper\
\
6/25:\
\
met with Brandt, got questions answered\
\
6/27:\
\
tried to do better job with equivalent widths.\
changed code to find the TWO wavelength elements closest to peak.\
was using fractional error without multiplying by value; now errors look way too small.\
\
6/30:\
\
tried again to get C code for SFD maps to work\
added header stuff to deal with getline() function duplicate\
fixed makefile so \'93make all\'94 works\
now getting \'93abort trap: 6\'94 errors, trying some print statements\
\
7/5/19:\
\
traced abort trap error to a sprintf() statement.\
it was trying to assign 8-character string + null to 8-character char array\
now pNaxis is correct, which is good. output is nonzero, but garbage\'85\
\
7/6/19:\
\
removed check for \'93little_endian.\'94\
output is now good; see \'93verify_SFD_values.py\'94 for scatterplot.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 ./dust_getval infile=infile.txt outfile=outfile.txt map=I100 interp=y
\f0\fs24 \cf2 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
7/8/19:\
updated equiv_width.py, not as many walls of code\
found bug where wrong std devs were displaying for unbinned spectra.\
Now IRIS errors are smaller, as expected.\
used actual std devs for continuum AND peak; values still seem off\
\
7/9/19:\
made text file with BOSS coordinates: BOSS_locations.txt\
	(using coordinates from sky_radec.dat)\
	(sky_radec.dat uses FK5 values, in degrees)\
ran C code on it to get SFD i100 at BOSS locations\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 ./dust_getval infile=\cf6 BOSS_locations\cf5 .txt outfile=SFD_i100_at\cf2 _BOSS_locations.txt \cf5 map=I100 interp=y\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 	note: dust_getval takes galactic coordinates (l, b)\
	ended up using BOSS_locations_galactic.txt in BrandtFiles folder\
tried to get IRIS values at BOSS locations (iris_i100_at_boss.npy)\
	Not consistent with the SFD values\'85\
\
7/10/19:\
fixed so BOSS values look good for SFD and IRIS\
(issues with coordinate conversion and bugs)\
made scatterplots (boss_comparison_71019, 3 images)\
	using verify_SFD_values.py\
verified that the skyfibers.dat and skyfibers_nativelam.fits Brandt just sent me are the ones I already had\
\
7/11/19:\
saved iris spectrum with correct std dev (iris_spectrum_71119.png)\
\
7/30/19\
counted 238906 lines in sky_radec.dat and in skyfibers.dat\
they seem to line up but the numbers in skyfibers.dat don\'92t even look like coordinates\
\
7/31/19:\
lambdas are given by default as log_10(lambda)\
spacing of start values is 10E-4, just like spacing within wavelength arrays\
one of the arrays starts quite a bit higher than the rest\
\
8/1/19:\
info on sky_radec.dat:\
	cols (0, 1, 2, 3, 4) are (mjd, plate, fiber, ra, dec)\
info on skyfibers.dat:\
	cols(0, 1, 2, 5, 6) are (mjd, plate, fiber), then stuff with redshift\
found error in my code to get taos for BOSS; was using wrong wavelength array\
fixed it and ran get_optical_depth\
	to get taos_boss.npy, optical depth at BOSS locations\
	avg is 0.12, probably close enough to 0.3\
\
previous meeting:\
drizzling: wavelength array is constant across a plate, so end up with ~4 spectra\
	each one comes from a subset of the plates.\
\
meeting (8/1/19)\
	drizzling: no need. looks like all the arrays line up\
	tread same plate on different days as separate plate\
\
8/5/19:\
using sky_radec.dat, NOT skyfibers.dat, because it doesn\'92t have coordinates\
no need for drizzling because wavelength arrays are all evenly spaced\
	(spacing within array is 10^-4 in log_lambda, same for spacing of first lambdas)\
verified that there is data from same plate on different days\
	lines 695, 696\
	695: date 55179, plate 3615\
	696: date 55208, plate 3615\
verified (again?) that the file BOSS_radec.dat is same as the sky_radec I\'92m using\
checked date range: 12-11-2009 to 06-29-2014 (about 1661 days)\
checked plate range: 3586 to 7565 (3979)\
\
8/6/19:\
created unique identifier: plate * 10000 + date mod 10000\
ran i100_with_tao.py to get 2d i100 arrays for boss: SFD and IRIS\
	i100_tao_boss.npy and i100_tao_boss_iris.npy\
	avg of i100_tao_boss is 1.7\
trying to create padded flambda array\
	(because the arrays start at different wavelengths)\
issue: flambda array has less rows than wavelength array (
\f2\fs26 \cf5 \cb0 238904 vs 238906)
\f0\fs24 \cf2 \cb3 \
\
8/7/19:\
in the process of organizing dust project folder, moving large files to terabyte drive\
	including i100_tao_small.npy and taos_small.npy, used to test limiting case of small taos (0.01)\
Brandt says wavelength array is just missing first 2 elements, which are same as third element\
padded array:\
	(BOSS_test.py)\
	pad flambdas with 0 so they don\'92t contribute to the sum\
	pad ivar with 1 so no division by 0\
	used numpy\'92s memmap to avoid filling up RAM\
	padded_flams_boss.npy, padded_ivars_boss.npy\
\
8/8/19:\
struggling with high memory usage in reproduce_figs.py;\
trying to use np.memmap to get around it.\
re-did the padding, made sure ivars padded with 1s\
created separate file: reproduce_figs_boss.py\
	verified that reproduce_figs.py still works after reversing changes\
\
8/9/19:\
modifying reproduce_figs_boss so it should never exceed RAM\
now works on non-BOSS data, doesn\'92t exceed 1.5 GB of RAM\
compare time vs. original code for non-BOSS:\
	1d: 1:22 vs. 0:52 (slower now)\
	2d: 3:17 vs 2:40 (slower now), but old code hit 14 GB of RAM, new stays under 1.5 GB\
ran 1d boss, got image\
	boss_1st_attempt and boss_1d_1st_attempt\
	again, clearly wrong.\
\
8/12/19:\
checked that padded ivar and flambda arrays load properly\
make histograms of SFD i100:\
	i100 distributions check out (peak around 2, go to around 8-10)\
histograms of ivar:\
	looked at wavelengths 4500 - 4600\
	(BOSS: peak around 6, go to 20-25)\
	(SDSS: peak around 5, go to 15-20)\
histograms of flamda:\
	looked at wavelengths 4500 - 4600\
	(BOSS: peak around 0, goes from -1 to 1.5)\
	(SDSS: peak around 0, goes from -3 to 3)\
\
8/13/19:\
trying some things in reproduce_figs_boss.py, using just 5 columns of the arrays (to make it faster)\
also tried averaging the 5 columns, in case it is a resolution issue. No luck.\
	also tried with 20 columns\
tried shifting wavelength array back, virtually no difference.\
tried changing threshold (currently 10)\
	(using avg of 5 columns)\
	change to 3: alpha went from -0.01 to -0.2\
	change to 30: alpha went back to -.01\
tried IRIS 1d instead of SFD, did not help\
	(as expected, i100 values are close to the same)\
verified SFD i100 values look good\
note: log_10(lambda) goes from 3.5496 to 4.0463\
tried chunks of rows\
	boss_first_10000_81319, boss_start_100k_81319, boss_start_200k_81319\
checked the padding, looks good\
\
8/14/19:\
tested with subset (first 10000) of SDSS locations. much more blurry but looks ok.\
ivar and flambda both have a bunch of elements =0, but shouldn\'92t be an issue\
	because those terms will be zero in the calculations.\
verified that starting lambda is constant across each plate\
\
8/15:\
Brandt says there was an indexing issue, sent new data\
	(skyfibers_lam0.fits - skyfibers_lam9.fits)\
columns of new data:\
	0: flambda\
	1: ivar\
	2: lambda\
	3: RA\
	4: dec\
	5: mjd\
	6: plate\
	7: fiber\
modifying reproduce_figs_boss to support the 10 separate files\
alphas now positive, but still having issues\
	they are mostly too small\
note: only first file has correct RA, plate number, etc.\
\
meeting notes (8/15/19):\
	we expect BOSS flambdas to be closer to 0 (by factor of 4/9) because the fibers\
	are smaller. this is what we see.\
\
8/16/19:\
RA and Dec values check out, i100 should be correct\
modifications to reproduce_figs_boss.py\
	now it works on the original SDSS data\
	the plotting is its own function\
got full spectrum for BOSS 1d\
	boss_fig3_81619.png\
tried removing memmaps; took up too much RAM\
I expected higher errors towards the edges because less data there.\
	this is what we see.\
\
8/19/19:\
checked padding to make sure it\'92s the same for all fibers on a given plate.\
ivar histogram:\
	lots of variation between sections, but generally peaks at 5-10 and goes to 20-30\
flambda histogram:\
	all centered at 0, tails generally went a bit past -1 and 1\
slight modifications to equiv_width so it works with BOSS\
ran equiv_width_update.py to get zoomed-in spectra\
	looks better than expected, makes me question the binned spectra\
updated to use sections of i100 (because flambda is divided into sections)\
save some figures:\
	boss_spectra_tau_81919\
	\
8/20/19:\
tried shifting the binning\
	sometimes made one of the peaks smaller, but overall shape was same\
found 6 plates with avg value >10:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf2 \cb7 	72576658, 72586605, 72596603, 72606679, 72626659, 72626683
\f0\fs24 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 tried removing plates with large values\
	alphas_boss_82019, alphas_boss_iris_1d_82019, and correspondingstds\
	didn\'92t have a big effect, doesn\'92t seem like bad plates are throwing off the results\
	biggest spike: stays at 4.15\
tried lowering masking to 5\
	in case IRIS values are getting cut off because they are bigger\
	alphas_boss_82019_2, etc.\
more thresholds:\
	15 is _3, 7.5 is _4, 12.5 is _5\
moved plotting function from equiv_width.py to generate_plots.py\
	\
meeting notes 8/20/19:\
	try different masking\
	figure out which i100 values are causing the discrepancy\
	look at typical star spectra for comparison\
	doesn\'92t know where 2.96 arcsec came from\
\
8/21/19:\
plotted 5 thresholds each for SFD and IRIS\
	sfd_thresholds_82119, iris_thresholds_82119\
	iris data has 3rd peak to left for all threshold values\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs26 \cf5 \cb0 6302.31347656
\f0\fs24 \cf2 \cb3  is wavelength of biggest anomaly (index 2499)\
	6th plate starts at index 2350, so index 149 of that file\
ran reproduce_figs_boss on just this wavelength\
	result: 4.15 alpha\
	(for IRIS, alpha = -0.29)\
	looked at histograms of xxsig and yxsig (and avgs over wavelength)\
compared xxsig and yxsig between SFD and IRIS\
	SFD histograms: xxsig_82119, xxsig_mean_82119, etc.\
	no obvious difference in histograms of values or plate averages\
looked at numerator and denominator for alpha\
	denominator (sum 1) is bigger for IRIS, as expected (160 vs 140)\
	numerator makes no sense: -47 for IRIS, 472 for SFD\
saved xxsig and yxsig\
	xxsig_82119.npy, xxsig_iris_1d_82119.npy, etc.\
found at least one difference of >500 in numerator\
	index: 3178\
\
8/22/19:\
nothing special about this plate; 682 larger, 1763 smaller (similar for IRIS)\
try removing negative i100 values\
	didn\'92t seem to have any effect\
try setting ivars of just index 3178 to 0\
	both ended up near 0.5\
difference is not abnormally large (it\'92s 1; 2 would be a bit large)\
	and both pretty close to the average for the respective data sets\
try removing index 3178 for whole spectrum\
	alphas_boss_82219.npy, alphas_boss_iris_1d_82219.npy, etc.\
	looks like it worked; plotted binned and unbinned\
y is large: -342000 (most y are between -3000 and 3000)\
ivar is large: 930 x 10^-7 (most ivar are < 2x10^-7)\
most x between -200 and 200, so -13.4 vs. 3.6 is relatively close\
conclusion: an overly large value of flambda with overly small variance\
looked at other large y-values\
	5 are more extreme; all 10 highest and lowest had ivar < 3*10^-9 (some were 0)\
double checked whether flambda values were anomalous\
	flam was -57, most were in range -5 to 5\
	i100 is 2.2, very typical\
generate 2d plots (with and without IRIS):\
	alphas_boss_2d_82219.npy\
\
8/23/19:\
saved boss spectrum figures:\
	boss_tau_spectrum_82319, boss_binned_spectra_82319, etc.\
started generating 1d plots for various thresholds\
	alphas_boss_82319_5.npy (threshold 5), etc.\
	various_thresh_boss_82319.png\
\
8/25/19:\
generated plots for IRIS at various thresholds\
	various_thresh_boss_iris_82519\
	look same as SFD plots\
	the dip for threshold=5 does n ot appear to be an anomaly\
	my interpretation: going right, correlation decreases because the model is worse.\
		going left, correlation decreases because we mask areas with a lot of dust.\
\
8/26/19:\
looked at flambda for other wavelengths at index 3178\
	they all seemed pretty big\
see if plate was a unique identifier for SDSS data:\
	hdulist is plate, wavelength, flambda, ivar (nothing extra)\
	fiberinfo: (2, 3, 4) are (l, b, i100). (0, 1) look like (plate, fiber)\
	\'93plate\'94 same for both: looks like (mjd mod 1000) then plate number\
	
\f2\fs26 \cf5 \cb0 [ 266602  266602  266602 ... 2974592 2974592 2974592]
\f0\fs24 \cf2 \cb3 \
division by 0 is happening because ivar of last wavelength are all 0\
tried removing negative i100 for threshold 5\
	initially looked like spikes were lower\
	but turns out I set those i100 to 0, which made x nonzero\'85\
\
8/27/19:\
checked why binned plots from reproduce_figs_boss.py look slightly off, especially peaks\
	saved new spectrum: alphas_boss_82719\
	turns out alphas are exactly the same, just slightly different binning\
	even when binning is fixed, and alphas are identical, there is a small discrepancy\
fixed bug where whole i100 was loading into RAM (to convert to float32)\
found no negative i100 (checked 2D and SDSS)\
moved reproduce_figs.py to old_code\
	(version that couldn\'92t handle large files)\
	made sure reproduce_figs_boss works on SDSS data\
verified that i100 divisions are working properly\
cleaned up one_wavelength_boss.py\
compare BOSS stds:\
	they are a little bigger for shorter wavelengths, a little smaller for bigger wavelengths\
BOSS binning:\
	definitely changes (main peaks are tall/thin vs. short/broad)\
	but 4000 A break and Mg/Fe dip are obvious no matter how the binning is done\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 meeting notes 8/27/19\
	overview: one of the flambda values was very large, and its ivar was also too large (by 100x +)\
		[changing threshold didn\'92t fix]\
		[x was also pretty different, but IRIS and SFD both unremarkable]	\
	alphas probably larger because of locations\
	yes, \'93plate\'94 is unique identifier for SDSS. mod 10000 is ideal\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
8/28/19:\
i100 2d sanity check\
	made scatterplots of old vs. new i100 for BOSS (chose 2 wavelengths, otherwise too much data)\
	new values are a bit smaller, as expected\
\
8/29/19:\
see previous day\
	made color-coded plot according to value of tao\
	confirmed that bigger i100 have bigger tao (more dust)\
	boss_i100_comparison_82919 (pink is high tao, blue is low)\
thought I found error in unit conversion, but just 2.96 vs. 3 arcsec\
	resulted in slight correction to BOSS unit conversion factor (<1 percent)\
trying a few masking methods:\
	mask all plates that contain fibers with i100>10\
		peaks go down a bit, but binned spectrum goes down a lot\
	mask all plates with AVERAGE > 10, plus fibers with i100>10\
		everything goes up (peaks and binned spectra)\
		also tried avg>15\
	mask all fibers with >10, and all plates with fibers >20 (everything goes up)\
	masked top 5 highest-avg, spectrum went up a bunch\
\
8/30/19\
confirmed that masking high values increases alpha\
reproducing original plots:\
	if I mask all fibers <10 and all plates <10, and divide by 1.38, reproduction looks identical\
	alphas_sdss_83019.npy [file is not divided by 1.38]\
run equiv_widths_update on spectra with plate (avg >10) masked\
	so far values seem to get slightly worse\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 meeting notes 8/30/19\
	removing whole plates should not affect the correlation\
	if linear model is true, masking should NOT introduce bias\
		but if not, it SHOULD\
	error on i100 just results in a bias factor. offset on i100 will be subtracted out.\
	flambda does NOT need to be corrected\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
9/2/19:\
fiddling with equivalent width on the 6585 NII line\
	previously my result was 5.32\
	now it\'92s 4.42\
	change to weighted mean: 4.438\
\
9/3/19:\
plotted spectrum from paper on top of mine. lines up almost exactly\
test equiv_width directly on those alphas:\
	too small generally\
	switched to median, almost no difference\
try changing continuum level, that works for individual lines\
looks like NII line is using continuum from OIII\
	IMPORTANT: turns out I had made a mistake, didn\'92t use delta lambda in calculation\
look up actual line wavelengths:\
	H-beta: 4861.4\cf2 \cb3 . OIII: 4959\cf2 \cb3 \
	OIII: 5007. H-alpha: 6564.5\
	\cf2 \cb3 NII: 6548. NII: 6583. SII: 6716.5. SII: 6731\cf2 \cb3 \
	(vacuum vs. air?)\
	peaks generally correspond to listed wavelengths from paper, except 4863 -> 4862\
major takeaway: my method of finding equiv. width is very dependent on which pts are chosen.\
	I\'92ll try fitting a gaussian instead\
tried weighted avg and median again, barely any effect\
implemented gaussian fitting to find equiv width\
	all the fits look good\
	also tried Lorentz shape\
	values way higher\
found bug, had switched H-alpha and H-beta corrections\
\
9/4/19:\
got OIII line widths to match pretty closely\
	when the 6 wavelengths are leftward of the peak\
H-beta is close but low (rightward of peak)\
6000s peaks:\
	all pretty close, but some need to be shifter right, others left\
	6585, for example, highest I can get is 6.5. 6550 highest is 2.2.\
I have to assume he used a slightly different method from me.\
Fixed formula for std dev of each bin, and updated ranges for calculating continuum stdevs\
	also fixed calculation of error on continuum stds. I was taking a straight average of the stds.\
std devs mostly look good now\
checked error I introduced by assuming delta_lambda const. for each peak\
	around 0.1 percent\
fixed error in integration of gaussian\
	I neglected to subtract the continuum after integrating\
	much better than before but still worse agreement with brandt paper\
\
9/5/19:\
spent a lot of time thinking about the best method for masking\
\
My thoughts on masking:\
	\
	\cf2 \cb3 General thoughts:\
		removing a plate:\
			if same correlation as rest, no effect\
			if large values and not masked, will make alphas smaller\
			[I think this is the best way to avoid bias]\
		plates with large average:\
			high values are far off just because the model is not ideal\
			low values are far off because the average is far off because lots of high values\
			(and if not a lot of high values but avg still high, avg still way off from ideal value)\
			these have to be masked completely\
		masking a fiber (but keep it in the average):\
			Masking single fibers only makes the correlation better.\
			But if avg is high, there is bias, and correlation can only get so good.\
			[side note: should not introduce bias if linear model true]\
		a few really high values throwing off the average\
			even if average is small, need to mask whole plate\
			this should only be an issue if those values are quite a bit larger than the other thresholds, and there are enough of 			them\cf2 \cb3 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \cb3 	Masking: a concise explanation:\
		large values will be too large because the model is not ideal\
			so mask large values\
			that should pretty much always make the result more accurate\
		but there is still the offset from the average\
			[see my diagrams]\
			this could apply to large OR small averages, only depends on difference between actual and ideal\
			[although large averages will necessarily be off by quite a bit, small ones might not]\
		\
	My Suggestion:\
		mask fibers above [threshold]\
		mask plates where elements above threshold have large impact\
			add up excesses over the threshold\
			mean shift should be close to proportional to this\
			[this should take care of masking plates with large average]\
\
	Quantitative way to decide threshold values\
		threshold value\
			should be same for masking whole plates and individual fibers\
			it\'92s the value where a large fraction (half?) of the i100 values can be there and fractional systematic error\
				in alpha is not too big (more than some amount)\
			so find impact on alpha from bias in i100 values\
				method 1: find d(alpha)/d(x_i) and multiply by i100 diff.\
				method 2: introduce (or subtract) a bias and see what happens \
				or both?\
			for indiv: don\'92t want to lose too much data, so value should be highest that doesn\'92t cause too much alpha error\
		whole plate:\
			impact of individual fibers on avg\
				[add up excesses, should be ~proportional] [correction bc factor is concave?]\
				[then correct to get actual offset of avg]\
			impact of avg on alpha\
				try changing it and see what happens\
		what alpha error is allowed?\
			probably want fractional error less than typical random error\cf2 \cb3 \
\
9/6/19:\
met with brandt\
(add notes)\
\
9/10/19:\
equiv width: tried integrating with trapezoids. still more or less agrees.\
	I think it\'92s close enough now.\
added weighted avgs back in. not much difference.\
note: in plots, right edge of bar corresponds to relevant wavelength\
was able to fit gaussian and continuum together for larger NII line\
	(using 10 sigma integration range)\
	equivalent width is 6.467, close to max value of 6.45 I got by adding bins,\
	smaller than brandt\'92s value of 6.6\
found out std devs have to be scaled for flux calibration also\
ran code to generate spectra for various thresholds, 1d, SFD, sdss\
	filenames: alphas_91019_10.npy, etc.\
\
9/11/19:\
generated spectra for various thresholds, 2d, SFD, sdss\
	alphas_2d_91119_10.npy\
	alphas_boss_91119_10.npy (1d, SFD, boss)\
	alphas_boss_iris_1d_91119_10.npy (1d, IRIS, boss) (on ahab now)\
	alphas_boss_iris_91119_10.npy (2d, IRIS, boss) (on ahab now)\
copying files to ahab:\

\f2\fs26 \cf5 \cb0 scp reproduce_figs_boss.py blakechellew@ahab.physics.ucsb.edu:python_scripts
\f0\fs24 \cf2 \cb3 \
alphas from brandt paper (I put figures in matplotlib using calc_temp.py,):\
	alphas = [0.18972603, 0.18287671, 0.1630137, 0.14657534, 0.09863014, 0.1609589, 0.18150685, 0.1890411, 0.16986301,\
		0.17328767, 0.20547945, 0.20205479, 0.20205479, 0.23835616, 0.26438356, 0.24383562, 0.20136986, 0.16986301, \
		0.1390411, 0.13630137, 0.17876712, 0.1739726, 0.28150685, 0.53835616, 0.61438356, 0.49452055, 0.31780822, \
		0.17465753, 0.19041096, 0.16643836, 0.16849315, 0.16164384, 0.19383562, 0.19520548, 0.1760274, 0.18356164, 		0.24109589, 0.33630137, 0.44315068, 0.4, 0.25, 0.20616438, 0.1760274, 0.16780822, 0.17123288, 0.16712329, \
		0.17671233, 0.17260274, 0.17328767, 0.20205479, 0.17671233, 0.17191781, 0.17808219, 0.20479452, 0.19383562,\
		0.1739726, 0.19383562, 0.19863014, 0.18630137, 0.19109589, 0.19246575, 0.18013699, 0.17945205, 0.17671233, 		0.16917808, 0.17260274, 0.16369863, 0.17671233, 0.18013699, 0.19726027, 0.16643836, 0.18561644, 0.1739726, \
		0.17123288, 0.16712329, 0.18219178, 0.18630137, 0.17945205, 0.18287671, 0.17808219, 0.1760274, 0.19383562,\
		0.16506849, 0.14931507, 0.16506849, 0.16575342, 0.17671233, 0.18630137, 0.18356164, 0.17534247, 0.18561644,\
		0.1869863, 0.1609589, 0.17808219, 0.19109589, 0.18561644, 0.13424658, 0.17191781, 0.17945205, 0.18561644, \
		0.17534247, 0.1869863, 0.17945205, 0.17739726, 0.17808219, 0.18630137, 0.18356164, 0.19726027, 0.20342466,\
		0.17739726, 0.15890411, 0.17260274, 0.17945205, 0.17534247, 0.18082192, 0.16849315, 0.18219178, 0.20205479,\
		0.19794521, 0.16712329, 0.16849315, 0.18493151, 0.17808219, 0.19520548, 0.17876712, 0.18835616, 0.19246575,\
		0.26712329, 0.37671233, 0.40410959, 0.3239726, 0.21849315, 0.17465753, 0.19041096, 0.19383562, 0.18287671,\
		0.20273973, 0.26643836, 0.33972603, 0.34383562, 0.26712329, 0.19863014, 0.17945205, 0.1869863, 0.19657534,\
		0.17260274, 0.18493151, 0.16369863, 0.17876712, 0.20273973, 0.20890411, 0.1869863, 0.18013699, 0.16849315,\
		0.16643836, 0.18493151, 0.19041096, 0.17260274, 0.17739726, 0.17328767, 0.15958904, 0.18287671, 0.19657534,\
		0.18561644, 0.19178082] #from Brandt paper\
	alphas = np.pad(alphas, pad_width=((2449, 1386)), mode='constant')\
made some threshold plots\
	sdss_thresholds_1d_91119\
	boss_thresholds_iris_1d_91119\
\
compare speed of AHAB\
	ran reproduce_figs_boss.py (1d, boss).  6:40 (ahab) vs. 7:20 (my computer)\
going through reproduce_figs code on ahab, getting rid of memmaps / for loops\
	now it takes 0:51\
	even for 2d model, it\'92s <2 min per spectrum\
compared to verify that code modified for ahab works properly. looks like it does.\
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 remember to change the masking back to 10\
remember to scale with flux calibration factor\
\
next:\
	ahab: why so many more \'93sum1 and sum2 are 0\'94?\
	check if any other parts I can speed up for ahab\
	make 2D threshold plots for BOSS\
	bootstrapping (look at 250 notes)\
	sync ahab version of reproduce_figs\
	try much larger values for masking [OR make all the plots with other masking, still illustrates the point pretty well]\
	\
	generate threshold plots (combine?)\
	finish transfering files to ahab and modify reproduce_figs to work with new file structure\
	generate spectra with all 8 main combinations (all threshold 10)\
	later, generate threshold plots for BOSS\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \cb3 threshold plots\
	try slightly higher thresholds\
	should do on boss/iris?\
		I can first show effect of iris/tao with sdss data\
		Then show effect of switching to boss\
		then show thresholds on boss/iris data\
		[should probably do both, then show one, maybe sdss, and say other was same\'85]\
	consider making more plots closer with masking values closer together\
	clean up code for these plots. it\'92s not very scalable\
	write helper code to loop through the various masking values and generate alphas.\
	use same masking: mask >10 and avg >10\
	1d plots: correlation gets worse as raise mask. we know bc of tau bc one side drops faster.\
	2d plots: we should see these effects reduce\
		then maybe we can increase the threshold	\
	see how long generating these plots takes on Brandt\'92s computer\cf2 \cb3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
brandt suggested:\
	applying for NSF\
	looking into Arizona (both) and UNMLV\
\
next steps:\
	correction factor for flux calibration\
	equivalent widths:\
		clean up code\
		make final decision on OIII range and how to choose which 6 elements\
		if it works, re-evaluate some of my claims from 134 report\
		and possibly reproduce the figures\
		find equiv width of He line\'85\
		still need to do error prop on the H-alpha corrections\
	bootstrap resampling to make error bars on spectra\
		at least set up the code to handle it, even if I haven\'92t decided the exact type of bootstrap\
	set up brandt\'92s computer to do the bootstrap\
	sky fiber density plot (have to do density map?)\
	look at different regions of the sky\
	raise limit on optical thickness/mask\
	some areas of the sky are contributing too much to the signal\
\
equiv_width next steps:\
	set it up to find equiv_width of the other lines (specify ranges)\
	include width of the 3727 line, and the Helium line\
	include error prop from 400 A break\
	check through the code\
	get equivalent widths for the various sdss correlation spectra\
	then find the equivalent boundary values for BOSS spectra\
	then get equivalent widths for the BOSS spectra\
\
	is gaussian a good shape? \
	make sure I have the correct alphas in there\
	modify to compare BOSS to non-BOSS\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 can try masking by tau, there are some temperature effects there (column density *T^6 . . .)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
set up computer\
	[make sure it\'92s storing big files to the right place]\
	\cf2 \cb3 nohup processname& (instead of SSH)\
	store files on the bigger hard drive\
	need to install conda. use rsync or something to transfer files.\
	back up files and stuff\
	logging in: 
\f2\fs26 \cf5 \cb0 ssh ahab.physics.ucsb.edu
\f0\fs24 \cf2 \cb3 \
	saving public key:\
		eval \'91ssh-agent\'92\
		ssh-add\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 bootstrapping:\
	[mask whole plates or individual fibers?]\
	figure out which variant is the best\
	find an example of it being used for astronomy\
	figure out if/why it is the best option for calculating errors\
	why better than other errors? (because we don\'92t know how those were calculated?)\
		(or, probably: those errors are for THOSE locations. we want to know what\
		would happen if we chose different locations)\
		(but is it actually better regardless?)\
	understand the theory (when it it justified; reliablility) (see \'93numerical recipes\'94)\
	decide how many samples to take \
		(100 is probably computationally prohibitive, depending on how many per sample)\
		try it on a few wavelengths, see how it scales/\
	[need block bootstraping to avoid destroying correlation?]\
	check out the bootstrap textbook and Efron\'92s 1979 paper\
	brandt: \
		bootstrap should converge like sqrt(n) (verify)\
		might be fine to do every 10th wavelength or so\
		consider binning first, see how accurate that would be\
	correlation not an issue right? \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 		\
look at typical star spectrum for comparison\
\
think about how else to check for bad data\
	check for bad y at other wavelengths\
	what about i100 with large diffs? prob large errors, and masked\'85\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
code mods:\
	clean up: one_wavelength_boss\
	structure generate_plots better\
	reproduce_figs: clean up, check for bugs\
	make sure all scripts to work with the new file structure\
	check through iris code\
\
dust paper:\
	read section 4\
	question: flux conversion factor of 1.38\
	finish summarizing paper (sections 5 and 6) [see drive]\
	look through it some more\
\
astro textbook:\
	stuff about relative line widths (see notes on paper)\
\
plot densities on sky location map\
\
clean up / organize \'93dust project\'94 folder\
look through onetab, get rid of excess tabs\
\
other sources:\
	read carroll and ostlie\
	check Q/A on drive\
	
\f3 \cf0 \cb1 \CocoaLigature1 review 250 (chai squared, max likelihood)\
	
\f0 \cf2 \cb3 \CocoaLigature0 follow up with some 134 stuff (see google drive, gauchospace)\
\
IRIS code\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf2 \CocoaLigature1 {\listtext	\uc0\u8226 	}[make coordinate frame a parameter, not hard coded (euler())]\
{\listtext	\uc0\u8226 	}\CocoaLigature0 find out why changing WCS origin to 0 made it work.\
\ls1\ilvl0\CocoaLigature1 {\listtext	\uc0\u8226 	}\CocoaLigature0 clean up code. also time and speed up. (checking for bad values\'85) (also clean up code for equiv_width)\
\ls1\ilvl0\CocoaLigature1 {\listtext	\uc0\u8226 	}\CocoaLigature0 figure out why my ad2xy doesn\'92t work (it seemed to work for file 198\'85)\
\ls1\ilvl0\CocoaLigature1 {\listtext	\uc0\u8226 	}figure out the cos(delta) thing\CocoaLigature0 \
\ls1\ilvl0\CocoaLigature1 {\listtext	\uc0\u8226 	}[\CocoaLigature0 method: track one point over different files]\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3 \cf0 \cb1 \CocoaLigature1 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
Notes relevant to paper:\
\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 \cb3 \CocoaLigature0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
Meetings:\
\
equivalent widths:\
	might be able to find value for 3727 peak\
\
which projection for sky locations (i tried mollweide)\
ask about the correction factor\
\
notes:\
	yes, value for 3727 peak would be good\
	fit gaussian together with line. some of the params are linear.\
	just use same masking (above 10, above 10 avg), doesn\'92t really matter because the first model is bad anyway\
	spectro-photometric offset (find it for BOSS)\
	try out trapezoidal integration\
	check how much the alphas are actually off by\
	better to think about it as f_lambda getting smaller (try thinking about limits this way)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \cb3 	\
\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \cb3 	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\b \cf2 \cb3 maybe:
\f0\b0 \cf2 \cb3 \
	why are 4000 A break and Fe dip so much bigger\
	ask about sanity checks\
		hard to do with such large files\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f4\b \cf2 last meeting:
\f0\b0 \
	[prep by looking into schools and then ask]\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx11832\pardirnatural\partightenfactor0
\cf2 	talk about statement of purpose, research proposal (how to decide)\
	any chance of funding for next year so I can stay on?\
	do you think I can contact any professors and see if they have availability for this year?\
	contacting professors at grad schools\
	ask about something in the book to show that I\'92ve been reading it}